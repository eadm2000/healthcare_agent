{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d556af5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:02:11.577855Z",
     "iopub.status.busy": "2025-04-15T18:02:11.577599Z",
     "iopub.status.idle": "2025-04-15T18:02:15.511874Z",
     "shell.execute_reply": "2025-04-15T18:02:15.510974Z"
    },
    "papermill": {
     "duration": 3.940622,
     "end_time": "2025-04-15T18:02:15.513115",
     "exception": false,
     "start_time": "2025-04-15T18:02:11.572493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9b695a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:02:15.522184Z",
     "iopub.status.busy": "2025-04-15T18:02:15.521736Z",
     "iopub.status.idle": "2025-04-15T18:02:17.330153Z",
     "shell.execute_reply": "2025-04-15T18:02:17.329607Z"
    },
    "papermill": {
     "duration": 1.814342,
     "end_time": "2025-04-15T18:02:17.331623",
     "exception": false,
     "start_time": "2025-04-15T18:02:15.517281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e025203f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:02:17.339940Z",
     "iopub.status.busy": "2025-04-15T18:02:17.339490Z",
     "iopub.status.idle": "2025-04-15T18:02:17.342909Z",
     "shell.execute_reply": "2025-04-15T18:02:17.342434Z"
    },
    "papermill": {
     "duration": 0.008537,
     "end_time": "2025-04-15T18:02:17.343928",
     "exception": false,
     "start_time": "2025-04-15T18:02:17.335391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#API key for Gemini\n",
    "api_key = \"ENTER_KEY\"\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1678b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:02:17.351978Z",
     "iopub.status.busy": "2025-04-15T18:02:17.351419Z",
     "iopub.status.idle": "2025-04-15T18:02:17.354675Z",
     "shell.execute_reply": "2025-04-15T18:02:17.354077Z"
    },
    "papermill": {
     "duration": 0.008323,
     "end_time": "2025-04-15T18:02:17.355764",
     "exception": false,
     "start_time": "2025-04-15T18:02:17.347441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I'm a girl and I'm riding the subway, coming back late after a party. I am in Paris. It seems to me that a man is stalking me and I am afraid of him. Give me instructions on what to do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9264ad19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:02:17.363327Z",
     "iopub.status.busy": "2025-04-15T18:02:17.363118Z",
     "iopub.status.idle": "2025-04-15T18:02:17.664373Z",
     "shell.execute_reply": "2025-04-15T18:02:17.663164Z"
    },
    "papermill": {
     "duration": 0.306315,
     "end_time": "2025-04-15T18:02:17.665462",
     "exception": true,
     "start_time": "2025-04-15T18:02:17.359147",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         )\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"API key not valid. Please pass a valid API key.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:34.128.171.17:443 {created_time:\"2025-04-15T18:02:17.556356508+00:00\", grpc_status:3, grpc_message:\"API key not valid. Please pass a valid API key.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/3781403256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = model.generate_content(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     generation_config={\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"top_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "    generation_config={\n",
    "        \"temperature\": 0,  \n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"response_mime_type\": \"text/plain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56d6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:16:31.123061Z",
     "iopub.status.busy": "2025-04-15T17:16:31.122506Z",
     "iopub.status.idle": "2025-04-15T17:16:48.102510Z",
     "shell.execute_reply": "2025-04-15T17:16:48.101704Z",
     "shell.execute_reply.started": "2025-04-15T17:16:31.123036Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Video\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc0a3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Dataset with audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfe608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:16:48.104392Z",
     "iopub.status.busy": "2025-04-15T17:16:48.103932Z",
     "iopub.status.idle": "2025-04-15T17:16:48.151456Z",
     "shell.execute_reply": "2025-04-15T17:16:48.150630Z",
     "shell.execute_reply.started": "2025-04-15T17:16:48.104373Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/kaggle/input/911-recordings/911_recordings/911_metadata.csv')\n",
    "print(data.columns.tolist())\n",
    "print(data['deaths'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72edbab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:16:48.153026Z",
     "iopub.status.busy": "2025-04-15T17:16:48.152356Z",
     "iopub.status.idle": "2025-04-15T17:16:48.173601Z",
     "shell.execute_reply": "2025-04-15T17:16:48.172948Z",
     "shell.execute_reply.started": "2025-04-15T17:16:48.153007Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cee48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:50:37.280452Z",
     "iopub.status.busy": "2025-04-15T16:50:37.279878Z",
     "iopub.status.idle": "2025-04-15T16:50:37.287275Z",
     "shell.execute_reply": "2025-04-15T16:50:37.286532Z",
     "shell.execute_reply.started": "2025-04-15T16:50:37.280428Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.set_index('id', inplace=True)\n",
    "\n",
    "print(data.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172c9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:00:34.941204Z",
     "iopub.status.busy": "2025-04-15T17:00:34.940422Z",
     "iopub.status.idle": "2025-04-15T17:01:50.112681Z",
     "shell.execute_reply": "2025-04-15T17:01:50.111846Z",
     "shell.execute_reply.started": "2025-04-15T17:00:34.941178Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers torchaudio librosa --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51053ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:16:48.175089Z",
     "iopub.status.busy": "2025-04-15T17:16:48.174910Z",
     "iopub.status.idle": "2025-04-15T17:16:48.190742Z",
     "shell.execute_reply": "2025-04-15T17:16:48.190128Z",
     "shell.execute_reply.started": "2025-04-15T17:16:48.175075Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/911-recordings/911_recordings/911_metadata.csv')\n",
    "\n",
    "row = data[data['id'] == 1].iloc[0]\n",
    "audio_filename = row['file_name']\n",
    "audio_path = os.path.join('/kaggle/input/911-recordings/911_recordings', audio_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b4df9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-15T17:09:25.507Z",
     "iopub.execute_input": "2025-04-15T17:08:37.828643Z",
     "iopub.status.busy": "2025-04-15T17:08:37.828371Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_audio, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "#Transcribe with Wav2Vec2\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "input_values = tokenizer(input_audio, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(\"Transcription:\\n\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea379d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct prompt\n",
    "prompt = f\"\"\"This is a transcription of a 911 call:\n",
    "\\\"\\\"\\\"\n",
    "{transcription}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "What do you understand about the emergency? Can you summarize what's happening and identify if any action needs to be taken?\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = model.generate_content(\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "    generation_config={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"response_mime_type\": \"text/plain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show the model's reply\n",
    "print(\"Gemini's response:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd258c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Only using description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc020440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:18:11.549020Z",
     "iopub.status.busy": "2025-04-15T17:18:11.548172Z",
     "iopub.status.idle": "2025-04-15T17:18:11.573727Z",
     "shell.execute_reply": "2025-04-15T17:18:11.573108Z",
     "shell.execute_reply.started": "2025-04-15T17:18:11.548987Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/911-recordings/911_recordings/911_metadata.csv')\n",
    "print(data.columns.tolist())\n",
    "print(data[['id', 'title', 'description']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bffbfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:19:13.425800Z",
     "iopub.status.busy": "2025-04-15T17:19:13.425130Z",
     "iopub.status.idle": "2025-04-15T17:19:13.432106Z",
     "shell.execute_reply": "2025-04-15T17:19:13.431253Z",
     "shell.execute_reply.started": "2025-04-15T17:19:13.425760Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "row = data[data['id'] == 128].iloc[0]\n",
    "\n",
    "row['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dd30d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:20:11.678966Z",
     "iopub.status.busy": "2025-04-15T17:20:11.678645Z",
     "iopub.status.idle": "2025-04-15T17:20:13.942758Z",
     "shell.execute_reply": "2025-04-15T17:20:13.941912Z",
     "shell.execute_reply.started": "2025-04-15T17:20:11.678940Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "This is a 911 call description. Please summarize the event and advise how to help:\n",
    "\n",
    "{row['description']}\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "    generation_config={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"response_mime_type\": \"text/plain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f429c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Another dataset with wide descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bb70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:42:11.944292Z",
     "iopub.status.busy": "2025-04-15T17:42:11.943375Z",
     "iopub.status.idle": "2025-04-15T17:42:15.172283Z",
     "shell.execute_reply": "2025-04-15T17:42:15.171415Z",
     "shell.execute_reply.started": "2025-04-15T17:42:11.944259Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/kaggle/input/montcoalert/911.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d80622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:42:48.227171Z",
     "iopub.status.busy": "2025-04-15T17:42:48.226406Z",
     "iopub.status.idle": "2025-04-15T17:42:48.232099Z",
     "shell.execute_reply": "2025-04-15T17:42:48.231332Z",
     "shell.execute_reply.started": "2025-04-15T17:42:48.227144Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def request_assistance_with_llm(row_index):\n",
    "    row = df.iloc[row_index]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an emergency assistant. Your job is to give calm, clear, step-by-step instructions \n",
    "    to someone in an emergency situation. You are not a chatbot. You never ask questions. \n",
    "    You respond with practical advice, immediately, based only on the information given.\n",
    "\n",
    "    If the user mentions danger to life, safety, or health, prioritize those in your instructions.\n",
    "    If possible, suggest calling emergency services or seeking nearby help, but always stay specific.\n",
    "\n",
    "    Situation:\n",
    "    {row['desc']}\n",
    "    \n",
    "    Instructions:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "        generation_config={\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40,\n",
    "            \"max_output_tokens\": 4096,\n",
    "            \"response_mime_type\": \"text/plain\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Response for case at {row['addr']}, {row['twp']}:\\n{response.text}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ad2fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:42:57.519216Z",
     "iopub.status.busy": "2025-04-15T17:42:57.518924Z",
     "iopub.status.idle": "2025-04-15T17:43:00.606836Z",
     "shell.execute_reply": "2025-04-15T17:43:00.606023Z",
     "shell.execute_reply.started": "2025-04-15T17:42:57.519195Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_assistance_with_llm(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f811d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### After it can be a dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972592a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Lets try falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec01c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:59:45.257206Z",
     "iopub.status.busy": "2025-04-15T17:59:45.256919Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/montcoalert/911.csv\")\n",
    "pipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "row_index = 1\n",
    "row = df.iloc[row_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b791e51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are an emergency assistant. Your job is to give calm, clear, step-by-step instructions \"\n",
    "    \"to someone in an emergency situation. You are not a chatbot. You never ask questions. \"\n",
    "    \"You respond with practical advice, immediately, based only on the information given.\\n\\n\"\n",
    "    \"If the user mentions danger to life, safety, or health, prioritize those in your instructions.\\n\"\n",
    "    \"If possible, suggest calling emergency services or seeking nearby help, but always stay specific.\\n\\n\"\n",
    "    f\"Situation:\\n{row['desc']}\\n\\n\"\n",
    "    \"Instructions:\"\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "response = pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0)\n",
    "\n",
    "# Print result\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Response:\\n{response[0]['generated_text'][len(prompt):]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3df19",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3483931,
     "sourceId": 6084669,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 100,
     "sourceId": 1381403,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.273379,
   "end_time": "2025-04-15T18:02:20.712157",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-15T18:02:07.438778",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
